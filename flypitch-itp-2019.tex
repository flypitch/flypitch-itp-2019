\documentclass[a4paper,USenglish,cleveref, autoref]{lipics-v2019}
%This is a template for producing LIPIcs articles. 
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling cleveref support, use "autoref"

\usepackage{color}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1}   % red
\definecolor{tacticcolor}{rgb}{0.1, 0.2, 0.6}    % blue
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green
\definecolor{attributecolor}{rgb}{0.7, 0.1, 0.1} % red
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean,breakatwhitespace,xleftmargin=\parindent}
\usepackage{mathabx}

\newcommand{\B}{\mathbb{B}}
\newcommand{\lil}{\lstinline}
\newcommand{\N}{\mathbb{N}}


%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{A formalization of forcing and the consistency of the failure of the continuum hypothesis} %TODO Please add

\titlerunning{A formalization of forcing and the consistency of the failure of the continuum hypothesis}%optional, please use if title is longer than one line

\author{Jesse Michael Han\footnote{Corresponding author.}}{Department of Mathematics, University of Pittsburgh \and \url{https://www.pitt.edu/~jmh288}}{jessemichaelhan@gmail.com}{}{}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional

\author{Floris van Doorn}{Department of Mathematics, University of Pittsburgh}{}{}{}

\authorrunning{J.\,M. Han and F.\, van Doorn}%TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Jesse Michael Han and Floris van Doorn}%TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[100]{Theory of computation~Logic and verification}
\ccsdesc[100]{Theory of computation~Type theory}
\ccsdesc[100]{Software and its engineering~Formal methods}
% \ccsdesc[100]{}%TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Interactive theorem proving, formal verification, set theory, forcing, independence, continuum hypothesis, Boolean-valued models, Lean}%TODO mandatory; please add comma-separated list of keywords

\category{}%optional, e.g. invited paper

\relatedversion{}%optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversion{A full version of the paper is available at \url{...}.}

\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{The authors would like to thank the members of the Pitt-CMU Lean group, particularly Simon Hudon, Jeremy Avigad, Mario Carneiro, and Tom Hales for their feedback and suggestions; we are also grateful to Dana Scott and John Bell for their advice and correspondence.}%optional

%\nolinenumbers %uncomment to disable line numbering

%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
We describe a formalization of forcing using Boolean-valued models in the Lean 3 theorem prover, including the fundamental theorem of forcing and a deep embedding of first-order logic with a Boolean-valued soundness theorem. As an application of our framework, we specialize our construction to a Boolean completion of the Cohen poset and formally verify in the resulting model the failure of the continuum hypothesis.
\end{abstract}

%TODO(jesse) maybe talk a bit more about the grand strategy for formalizing the independence of CH, i.e. all the prerequisites, and move the discussion about BVMs to the proof sketch section.
\section*{Introduction}
The continuum hypothesis states that there are no sets strictly larger than the countable natural numbers and strictly smaller than the uncountable real numbers. It was introduced by Cantor in 1878 and was the very first problem on Hilbert's list of twenty-three outstanding problems in mathematics. G\"odel proved in 1938 \cite{godel1938consistency} that the continuum hypothesis was consistent with $\mathsf{ZFC}$, and later conjectured that the continuum hypothesis was independent of $\mathsf{ZFC}$, i.e. neither provable nor disprovable from the $\mathsf{ZFC}$ axioms. In 1963, Paul Cohen developed \emph{forcing} \cite{cohen-the-independence-of-the-continuum-hypothesis-1}, which allowed him to prove the consistency of the negation of the continuum hypothesis, and therefore complete the independence proof. For this work, which marked the beginning of modern set theory, he was awarded a Fields medal---the only one to ever be awarded for a work in mathematical logic.

The work we describe in this paper is part of the Flypitch project\footnote{\url{https://github.com/flypitch/flypitch/}}, which aims to formalize the independence of the continuum hypothesis. Our results mark a major milestone towards that goal.

Our formalization is written in the Lean 3 theorem prover. Lean is an interactive proof assistant under active development at Microsoft Research \cite{de2015lean} \cite{sebastian-slides}. It implements the Calculus of Inductive Constructions and has a similar metatheory to Coq, adding definitional proof irrelevance, quotient types, and a noncomputable choice principle. There is a well-known encoding of $\mathsf{ZFC}$ into dependent type theory with CIC, due to Aczel and Werner, which has been implemented in Lean's mathematical components library \lil{mathlib}. The fact that Lean's metatheory is powerful enough to encode a model of $\mathsf{ZFC}$ already allows us to perform metatheoretic arguments about $\mathsf{ZFC}$ which were unavailable to e.g. Paulson \cite{paulson2003relative}, who went to extreme lengths to circumvent them inside Isabelle/ZF. While this was a formidable task, we content ourselves with treating $\mathsf{ZFC}$ as a mathematical object of study, and freely ignore the restrictions which it would impose as a foundation for the metatheory.

Indeed, our formalization makes as much use of the expressiveness of Lean's dependent type theory as possible, using constructions which are impossible or unwieldy to encode in HOL, much less ZF: Lean's ordinals and cardinals, which are defined as equivalence classes of well-ordered types, live one universe level up and play a crucial role in the forcing argument; the models of set theory we construct require as input entire universes of types; our encoding of first-order logic crucially uses parametrized inductive types to equate type-correctness with well-formedness, eliminating the need for separate well-formedness proofs.

Why Boolean-valued models? The method of forcing with Boolean-valued models was developed by Solovay and Scott (and independently, Vop{\v e}nka) in '65-'66 \cite{scott1} \cite{vopenka1} as a simplification of Cohen's method. Some of these simplifications were incorporated by Shoenfield \cite{solovay1} into a general theory of forcing using partial orders, and it is in this form that forcing is usually practiced. While both approaches have essentially the same mathematical content (see e.g. the discussion in Kunen \cite{kunen1} or Jech \cite{jech1}), there are several reasons why we chose Boolean-valued models for our formalization:

\begin{itemize}
\item \textbf{Modularity.} The theory of forcing with Boolean-valued models cleanly splits into several components (a general theory of Boolean-valued semantics for first-order logic, a library for calculations inside complete Boolean algebras, the construction of Boolean-valued models of set theory, and the specifics of the forcing argument itself) which could be formalized in parallel and then recombined.

\item \textbf{Directness.} For the purposes of an independence proof, the Boolean-valued soundness theorem eliminates the need to produce a two-valued model. This approach also bypasses any requirement for the reflection theorem/L\"owenheim-Skolem theorems, Mostowski collapse, countable transitive models, or genericity considerations for filters.

\item \textbf{Novelty and reusability.} As far as we were able to tell, the Boolean-valued approach to forcing has never been formalized. Furthermore, while for the purposes of an independence proof, forcing with Boolean-valued models and forcing with countable transitive models accomplish the same thing, a general library for Boolean-valued semantics of a deeply embedded logic could be used for formal verification applications outside of set theory, e.g. to formalize the Boolean-valued semantics of the stochastic $\lambda$-calculus \cite{scott2}.

  \item \textbf{Amenability to structural induction.} As with Coq, Lean is able to encode extremely complex objects and reason about their specifications using inductive types. However, the user must be careful to choose the encoding so that properties they wish to reason about are accessible by structural induction, which is the most natural mode of reasoning in the proof assistant. After observing (1) that the Aczel-Werner encoding of ZFC as an inductive type is essentially a special case of the recursive \emph{name} construction from forcing (c.f. Section \ref{sect:bvm}), and (2) that the automatically-generated induction principle for that inductive type \emph{is} $\in$-induction, it is easy to see that this encoding can be modified to produce a Boolean-valued model of set theory where, again, $\in$-induction comes for free.
\end{itemize}



%%%%% put at end of introduction
The rest of the paper is organized as follows. In Section \ref{sect:outline} we outline the method of Boolean-valued models and sketch the forcing argument. Section \ref{sect:fol} discusses a deep embedding of first-order logic, including a proof system, soundness and completeness theorems, and crucially Boolean-valued semantics and the Boolean-valued soundness theorem. Section \ref{sect:bvm} discusses our construction of Boolean-valued models of set theory, emphasizing the usefulness of being able to metaprogram custom tactics to simulate predicate calculus inside an arbitrary complete Boolean algebra. Section \ref{sect:forcing} describes the formalization of the forcing argument and the construction of a suitable Boolean algebra for forcing $\neg\mathsf{CH}$. Section \ref{sect:ccc} describes the formalization of the $\Delta$-system lemma, a technical result in transfinite combinatorics which ensures the preservation of cardinal inequalities. We conclude with an indication of future work towards a formal proof of the independence of the continuum hypothesis.
%%%%%
\section{Outline of the proof}
\label{sect:outline}
% A \emph{proof} of $\mathsf{CH}$ from $\mathsf{ZFC}$ is a finite list of deductions starting from $\mathsf{ZFC}$ and ending at $\mathsf{CH}$. We write $\mathsf{ZFC} \vdash \mathsf{CH}$ to indicate that there exists a proof of $\mathsf{CH}$ from $\mathsf{ZFC}$. It is easy to show that $\mathsf{ZFC} \vdash \mathsf{CH}$: one simply exhibits a proof. What is harder is showing that $\mathsf{CH}$ is unprovable: one has to rule out all possible proofs. If one is allowed to use $\mathsf{ZFC} \vdash \neg \mathsf{CH}$, it is easy to show that $\mathsf{CH}$ is unprovable. What is even harder is showing simultaneously that $\mathsf{CH}$ is independent of ZFC: both  $\mathsf{CH}$ \emph{and} $\neg \mathsf{CH}$ are unprovable.

% Separate from provability is satisfiability: we say that $\mathsf{ZFC}$ \emph{satisfies} $\mathsf{CH}$, written $\mathsf{ZFC} \models \mathsf{CH}$, if the interpretation of $\mathsf{CH}$ in any model of $\mathsf{ZFC}$ is true. Dually, it is easy to show that $\mathsf{ZFC} \not \models \mathsf{CH}$: one simply exhibits a model of $\mathsf{ZFC}$ where $\mathsf{CH}$ is false, and harder to show that $\mathsf{ZFC} \models \mathsf{CH}$, because one has to consider all possible models.

$\mathsf{ZFC}$ is a collection of first-order sentences in the language of a single binary predicate relation $\{\in\}$, used to axiomatize set theory. The continuum hypothesis can be written in this fashion as a first-order sentence $\mathsf{CH}$. A proof of $\mathsf{CH}$ is a finite list of deductions starting from $\mathsf{ZFC}$ and ending at $\mathsf{CH}$. % We write $\mathsf{ZFC} \vdash \mathsf{CH}$ to indicate that there is a proof of $\mathsf{CH}$. 
The soundness theorem says that provability implies satisfiability, i.e. if $\mathsf{ZFC} \vdash \mathsf{CH}$, then $\mathsf{CH}$ interpreted in any model of $\mathsf{ZFC}$ is true. Taking the contrapositive, we can demonstrate the unprovability (equivalently, the consistency of the negation) of $\mathsf{CH}$ by exhibiting a single model where $\mathsf{CH}$ is not true.

A model of a first-order theory $T$ in a language $L$ is in particular a way of assigning $\mathsf{true}$ or $\mathsf{false}$ in a coherent way to sentences in $L$. Modulo provable equivalence, the sentences form a Boolean algebra and ``coherent'' means the assignment is a Boolean algebra homomorphism (so $\land$ becomes meet, $\lor$ becomes join, $\forall$ becomes an indexed infimum, etc.) into $\mathbf{2} = \{\mathsf{true}, \mathsf{false}\}$. The soundness theorem ensures that this homomorphism $v$ sends a proof $\phi \vdash \psi$ to an inequality $v(\phi) \leq v(\psi)$. But $\mathbf{2}$ does not really play a special role in this scheme, and may be replaced by any complete Boolean algebra $\B$, where the top and bottom elements $\top, \bot$ take the place of $\mathsf{true}$ and $\mathsf{false}$. It is straightforward to extend this analogy to a $\B$-valued semantics for first-order logic, and in this generality, the soundness theorem now says that for any such $\B$, if $\mathsf{ZFC} \vdash \mathsf{CH}$, then for any $\B$-valued structure where all the axioms of $\mathsf{ZFC}$ have truth-value $\top$, $\mathsf{CH}$ does also. Then as before, to demonstrate the consistency of the negation of $\mathsf{CH}$ it suffices to find just one $\mathbb{B}$ and a single $\mathbb{B}$-valued model where $\mathsf{CH}$ is not ``true''.

This is where forcing comes in. Given a universe $V$ of set theory which contains a Boolean algebra $\B$, one constructs in analogy to the cumulative hierarchy a new $\B$-valued universe $V^\B$ of set theory, where the powerset operation is replaced by taking functions into $\B$. Thus, the structure of $\B$ informs the decisions made by $V^\B$ about what subsets, hence functions, exist among the members of $V^\B$; the real challenge lies in selecting a suitable $\B$ and reasoning about how its structure affects the structure of $V^\B$. While $V^\B$ may vary wildly depending on the choice of $\B$, the original universe $V$ always embeds into $V^\B$ via an operation $x \mapsto \check{x}$, and while the passage of $x$ to $\check{x}$ may not always preserve its original properties, $\Delta_0$-properties are always preserved; in particular, $V^\B$ thinks $\check{\mathbb{N}}$ is $\mathbb{N}$.

To force the negation of the continuum hypothesis, we use the Boolean algebra of regular opens of the Cantor space $\B := \operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$. For each $\nu \in \aleph_2$, we associate the $\B$-valued characteristic function $\chi_\nu : \mathbb{N} \to \B$ by $n \mapsto \{f \operatorname{|} f(\nu, n) = 1\}$. This induces what $V^\B$ thinks is a new subset $\widetilde{\chi_{\nu}} \subseteq \mathbb{N}$, called a \emph{Cohen real}, and furthermore, simultaneously performing this construction on all $\nu : \aleph_2$ induces what $V^\B$ thinks is a function from $\check{\aleph_2} \to \mathcal{P}(\mathbb{N})$. After showing that $V^\B$ thinks this function is injective, to finish the proof it suffices to show that $x \mapsto \check{x}$ preserves cardinal inequalities, as then we will have squeezed $\check{\aleph_1}$ properly between $\mathbb{N}$ and $\mathcal{P}(\mathbb{N})$. This is really the technical heart of the matter, and centers on a combinatorial property of $\B$ called the \emph{countable chain condition} (CCC), the proof of which requires a detailed combinatorial analysis of the basis of the product topology for $2^{\aleph_2 \times \mathbb{N}}$.

So far we have mentioned nothing about how this argument, which is wholly set-theoretic, is to be interpreted inside type theory. To do this, it was important for us to separate the mathematical content from the metamathematical content of the argument. Traditional presentations of forcing are careful to stay within the circle of ZFC, and it is not immediately clear what parts of the argument use that set-theoretic foundation in an essential way, e.g. if to perform forcing one must have ordinals that look like von Neumann ordinals in the metatheory, or if one must take a model of set theory as the starting point, or if $\B$ must be internal to a model of set theory, etc. As we will see, our formalization will clarify some of these questions.

Finally, we remark that for working with Boolean-valued models, it is profitable to keep in mind the following analogy, developed by Scott in \cite{scott1}. A ready supply of complete Boolean algebras $\B$ is obtained by taking the measure algebra of a probability space and quotienting by the ideal of events of measure zero. Let $\mathbf{M}$ be a $\B$-valued structure. A unary $\B$-valued predicate $\phi$ on $\mathbf{M}$ assigns an event to every element $m$ of $\mathbf{M}$, whose measure we can think of as being the probability that $\phi(m)$ is true. Specializing to the language of set theory, we can attach to every $m : \mathbf{M}$ an ``indicator function'' $\lambda x, x \in m$ which assigns to every $x$ a probability that it is actually a member of $m$. Thus, by virtue of extensionality, we may think of the elements of a $\B$-valued model of ZFC as being ``set-valued random variables'', or ``random sets''.\footnote{In this analogy, given a universe of random sets, the purpose of the generic filter or ultrafilter in forcing is then to simultaneously evaluate the outcomes of the random variables, collapsing them into an ordinary universe of sets.} We refer the interested reader to \cite{scott1} and \cite{moore1} for details.

\paragraph*{Sources}
Our strategy for constructing a Boolean-valued model in which the continuum hypothesis fails is a synthesis of the proofs in the textbooks of Bell (\cite{bell1}, Chapter 2) and Manin (\cite{manin1}, Chapter 8). For the $\Delta$-system lemma, which is required for the countable chain condition, we followed Kunen (\cite{kunen1}, Chapters 1 and 5).

\paragraph*{Viewing the formalization}
The code blocks in this paper should be taken as pseudocode. In some places, universe levels, type ascriptions, and casts have been removed to improve readability. We urge the interested reader to view our formalization.

The source code for our formalization is available at \url{https://github.com/flypitch/flypitch}. The forcing argument for the negation of CH is located in \lstinline{forcing.lean}. In a Lean-aware editor such as Emacs, the user is encouraged start at the theorem \lstinline{neg_CH} and jump backwards to trace the dependencies of the proof. (TODO(jesse) maybe change this to point at the ``summary'' file)


\section{First-order logic}
\label{sect:fol}
The starting point for first-order logic is a \emph{language} of relation and function symbols. We represent a language as a pair of $\N$-indexed families of types, each of which is to be thought of as the collection of relation (resp. function) symbols, but stratified by arity.

\begin{lstlisting}
structure Language : Type (u+1) :=
(functions : ℕ → Type u) (relations : ℕ → Type u)
\end{lstlisting}

\subsection{(Pre)terms, (pre)formulas}
% /- preterm L l is a partially applied term. if applied to n terms, it becomes a term.
% * Every element of preterm L 0 is a well-formed term.
% * We use this encoding to avoid mutual or nested inductive types, since those are not too convenient to work with in Lean. -/
TODO(floris) convert docstring into english

\begin{lstlisting}
inductive preterm : ℕ → Type u
| var {} : ∀ (k : ℕ), preterm 0
| func : ∀ {l : ℕ} (f : L.functions l), preterm l
| app : ∀ {l : ℕ} (t : preterm (l + 1)) (s : preterm 0), preterm l
\end{lstlisting}

TODO(floris) convert docstring into english
% preformula l is a partially applied formula. if applied to n terms, it becomes a formula.
%   * We only have implication as binary connective. Since we use classical logic, we can define
%     the other connectives from implication and falsum.
%   * Similarly, universal quantification is our only quantifier.
%   * We could make `falsum` and `equal` into elements of rel. However, if we do that, then we cannot make the interpretation of them in a model definitionally what we want.

\begin{lstlisting}
inductive preformula : ℕ → Type u
| falsum {} : preformula 0
| equal (t₁ t₂ : term L) : preformula 0
| rel {l : ℕ} (R : L.relations l) : preformula l
| apprel {l : ℕ} (f : preformula (l + 1)) (t : term L) : preformula l
| imp (f₁ f₂ : preformula 0) : preformula 0
| all (f : preformula 0) : preformula 0
\end{lstlisting}

TODO(floris) convert docstring into english

%   /- Provability
% * to decide: should Γ be a list or a set (or finset)?
% * We use natural deduction as our deduction system, since that is most convenient to work with.
% * All rules are motivated to work well with backwards reasoning.
% -/

\begin{lstlisting}
inductive prf : set (formula L) → formula L → Type u
| axm     {Γ A} (h : A ∈ Γ) : prf Γ A
| impI    {Γ : set (formula L)} {A B} (h : prf (insert A Γ) B) : prf Γ (A ⟹ B)
| impE    {Γ} (A) {B} (h₁ : prf Γ (A ⟹ B)) (h₂ : prf Γ A) : prf Γ B
| falsumE {Γ : set (formula L)} {A} (h : prf (insert ∼A Γ) ⊥) : prf Γ A
| allI    {Γ A} (h : prf (lift_formula1 '' Γ) A) : prf Γ (∀' A)
| allE₂   {Γ} A t (h : prf Γ (∀' A)) : prf Γ (A[t // 0])
| ref     (Γ t) : prf Γ (t ≃ t)
| subst₂  {Γ} (s t f) (h₁ : prf Γ (s ≃ t)) (h₂ : prf Γ (f[s // 0])) : prf Γ (f[t // 0])
\end{lstlisting}

\subsection{Completeness}
As part of our formalization of first-order logic, we completed a verification of the G\"odel completeness theorem. Although our present development of forcing did not require it, we anticipate that it will required later to e.g. prove the downward L\"owenheim-Skolem theorem to extract countable transitive models for forcing with generic extensions; also, like the soundness theorem, it serves as a proof-of-concept and a stress-test of our chosen encoding of first-order logic.

For our formalization, we chose the Henkin-style approach of constructing a canonical term model. In order to perform the argument, which normally involves modifying the language ``in place'' to iteratively add new constant symbols, we had to adapt it to type theory. Since our languages are represented by pairs of indexed types instead of sets, we cannot really modify them in-place with new constant symbols. Instead, at each step of the construction, we must construct an entirely new language, in which the previous one embeds as a subtype, and in the limit, we cannot take a union, but must rather compute a directed colimit of types. This directed colimit of languages induces similar constructions on preterms, preformulas, sentences, and so on, and completing the argument requires reasoning with all of them. However, our proof was made significantly easier by our design decisions: only a few arguments required anything more than straightforward case-analysis and structural induction.

Here is one notable exception. In the process of our formalization, we discovered a nontrivial hole in many presentations of the Henkin argument (TODO(floris) describe \lstinline{reflect_prf})

We remark that our formalization of the completeness theorem is as general as possible, making no assumptions on the cardinality of the language or the presence of function symbols.

\subsection{(Boolean-valued) soundness}
\subsection{Boolean-valued semantics for first-order logic}

A \textbf{complete Boolean algebra} is a type $\B$ equipped with the structure of a Boolean algebra and additionally operations $\operatorname{Inf}$ and $\operatorname{Sup}$ (which we write as $\bigsqcap$ and $\bigsqcup$) returning the infimum and supremum of an arbitrary collection of members of $\B$. For more details on complete Boolean algebras, we refer the reader to the textbook of Halmos-Givant \cite{halmos-givant1}.

\begin{definition}\label{def-boolean-valued-structure}
  A Boolean-valued structure is...
\end{definition}

Note that Boolean-valued equality is not really an equivalence relation. One complication which then arises in Boolean-valued semantics is keeping track of the extensionality proofs for formulas. However, as part of the Soundness theorem shows, once these extensionality proofs are provided for the basic symbols in the language, they extend by structural induction to all formulas.

\subsection{The soundness theorem}

A soundness theorem says that a proof tree may be replayed to produce an actual proof in the object of truth-values. When the object is truth-values is the type \texttt{Prop} of propositions, this says that a proof tree compiles to a proof term. When the object of truth-values is a Boolean algebra, this says that the proof tree becomes an internal implication from the interpretation of the context to the interpretation of the conclusion.

We designed our datatype of proofs as an inductive type whose constructors are precisely the natural deduction rules naturally supported by Lean's Prop. As a result, the proofs of either soundness theorem become straightforward structural inductions.

%% TODO(floris) elaborate a bit

\section{Constructing Boolean-valued models of set theory}
\label{sect:bvm}
Throughout this section, we fix a universe level $u$, a type \lstinline{𝔹 : Type u} and an instance of a complete Boolean algebra structure on $\B$.

In set theory (see e.g. Jech \cite{jech1} or Bell \cite{bell1}), Boolean-valued models are obtained by imitating the construction of the von Neumann cumulative hierarchy via a transfinite recursion where iterations of the powerset operation (taking functions into $\mathbf{2} = \{\operatorname{true}, \operatorname{false}\}$) are replaced by iterations of the ``\lstinline{𝔹}-valued powerset operation'' (taking functions into $\B$).

Since this construction by transfinite recursion does not easily translate into type theory, our construction of Boolean-valued models of set theory is instead a variation on a well-known encoding originally due to Aczel \cite{aczel1} \cite{aczel2} \cite{aczel3}. This encoding was adapted by Werner \cite{werner1} to encode ZFC into Coq, whose metatheory is close to that of Lean. Werner's construction was re-implemented in Lean's \texttt{mathlib} by Carneiro as part of \cite{mario1}. In this approach, one takes a universe of types \texttt{Type u} as the starting point and then imitates the cumulative hierarchy by constructing the inductive type
\begin{lstlisting}
inductive pSet : Type (u+1)
| mk (α : Type u) (A : α → pSet) : pSet
\end{lstlisting}
(Just as the empty set kicks off the construction of the cumulative hierarchy, the empty type admits an empty map into any type and so induces \lstinline{∅ : pSet}.)

The Aczel-Werner encoding is closely related to the recursive definition of \emph{names}, which is used in forcing to construct forcing extensions:

\begin{definition}\label{def-p-name}
Let $P$ be a partial order (which one thinks of as a collection of forcing conditions). A \emph{$P$-name} is a collection of pairs $(y, p)$ where $y$ is a $P$-name and $p : P$.
\end{definition}

 If $P$ consists of only one element, then a $P$-name is specified by essentially the same information as a member of the inductive type \lstinline{pSet} above. Conversely, specializing $P$ to an arbitrary complete Boolean algebra $\B$, we generalize the definition of \lstinline{pSet.mk} so that elements are recursively assigned Boolean truth-values:
\begin{lstlisting}
inductive bSet (𝔹 : Type u) [complete_boolean_algebra 𝔹] : Type (u+1)
| mk (α : Type u) (A : α → bSet) (B : α → 𝔹) : bSet
\end{lstlisting}
Thus \lil{bSet 𝔹} is the type of $\B$-names, and will be the underlying type of our Boolean-valued model of set theory. For convenience, if \lstinline{x : bSet 𝔹} and \lstinline{x := ⟨α, A, B⟩}, we put \lstinline{x.type := α, x.func := A, x.bval := B}.

\subsection{Boolean-valued equality and membership}
% In order to view \lil{bSet 𝔹} as a Boolean-valued structure, we must first equip it with a Boolean-valued notion of equality. In models of set theory, the axiom of extensionality ensures that the notions of \lil{=}, \lil{∈}, and \lil{⊆} are tightly connected, and in fact, in every reference we found, \lil{=} and \lil{∈} were defined simultaneously using mutual recursion. Although this makes checking e.g. the axiom of extensionality a matter of definition, we found this was unnecessary, and avoid the mutually recursive definition by making equality the primitive notion.

In \lil{pSet}, equivalence of sets is defined by structural recursion as follows: two sets $x$ and $y$ are equivalent if and only if for every $w \in x$, there exists a $w' \in y$ such that $w$ is equivalent to $w'$, and vice-versa. Analogously, by translating quantifiers and connectives into operations on $\B$, Boolean-valued equality is defined in the same way:
\begin{lstlisting}
def bv_eq : ∀ (x y : bSet 𝔹), 𝔹
| ⟨α, A, B⟩ ⟨α', A', B'⟩ :=
             (⨅a : α, B a ⟹ ⨆a', B' a' ⊓ bv_eq (A a) (A' a')) ⊓
               (⨅a' : α', B' a' ⟹ ⨆a, B a ⊓ bv_eq (A a) (A' a'))
\end{lstlisting}

We abbreviate \lil{bv_eq} with the infix operator \lil{=ᴮ}. With equality is place, it is easy to define membership, by translating ``$x$ is a member of $y$ if and only if there exists a $w \in y$ such that $x = w$.'' As with equality, we denote $\B$-valued membership with \lil{∈ᴮ}.
             
\begin{lstlisting}
def mem : bSet 𝔹 → bSet 𝔹 → 𝔹
| a (mk α' A' B') := ⨆a', B' a' ⊓ a =ᴮ A' a'
\end{lstlisting}

\subsection{Reasoning in $\B$}
As Scott stresses in \cite{scott3}, ``A main point ... is that the well-known algebraic characterizations of [complete Heyting algebras] and [complete Boolean algebras] exactly mimic the rules of deduction in the respective logics\ldots{}'' Indeed, that is really why the Boolean-valued soundness theorem is true. One thinks of the \lil{≤} symbol in an inequality of Boolean truth-values as a turnstile in a proof state: the conjunctands on the left as a list of assumptions in context, and the quantity on the right as the goal. For example, given \lil{a b : 𝔹}, the identity $(a \Rightarrow b) \sqcap a \leq b$ could be proven by unfolding the definition of material implication, but it is really just the natural deduction rule of implication elimination; similarly, given an indexed family \lil{a : I → 𝔹}, \lstinline{⨆i, a i ≤ b ↔ ∀ i, a i ≤ b} is just casing on an existential quantifier.

Where the difficulty arises with having only a basic library of lemmas like the ones above is when the statements one wants to prove become not even nontrivial, but only slightly more complicated. Consider the following example, which should be  ``\lil{by assumption}'':
\begin{lstlisting}
  ∀ a b c d e f g: 𝔹, (d ⊓ e) ⊓ (f ⊓ g ⊓ ((b ⊓ a) ⊓ c)) ≤ a
\end{lstlisting}
or slightly less trivially, the following example where the goal is attainable by ``just applying a hypothesis to an assumption''
\begin{lstlisting}
  ∀ a b c d : 𝔹, (a ⟹ b) ⊓ c ⊓ (d ⊓ a) ≤ b
\end{lstlisting}

There are three ways to deal with goals like these, which approximately describe the evolution of our approach. First, one can try using the basic lemmas in \lil{mathlib}, using the simplifier to normalize expressions, and performing clever rewrites with the deduction theorem\footnote{The deduction theorem in a Boolean algebra says that for all $a, b$ and $c$, $a \sqcap b \leq c \iff a \leq b \Rightarrow c$.}. Second, one can take the LCF-style approach and expand the library of lemmas with increasingly sophisticated derived inference rules.

Third, one can make the following observation:

\begin{lemma}\label{poset-yoneda}
  Let $(P, \leq)$ be a partially ordered set. Let $a \hspace{1mm} b : P$. Then $a \leq b$ if and only if $\forall \Gamma : P, \Gamma \leq a \to \Gamma \leq b$.
\end{lemma}
This is an instance of the Yoneda lemma for partially ordered sets, and its proof is utterly trivial. However, one side of the equivalence is much easier for Lean to reason with. Take the example which should have been ``\lil{by assumption}''. The following proof, in which the user navigates down the binary tree of nested \lil{⊓}s, will work:
\begin{lstlisting}
example {a b c d e f g : 𝔹} : (d ⊓ e) ⊓ (f ⊓ g ⊓((b ⊓ a)⊓ c)) ≤ a :=
by {apply inf_le_right_of_le, apply inf_le_right_of_le,
    apply inf_le_left_of_le, apply inf_le_right_of_le, refl}
\end{lstlisting}

But if we use the right-hand side of \autoref{poset-yoneda} instead, then after some preprocessing, \lstinline{assumption} will literally work:

\begin{lstlisting}
example {a b c d e f g : 𝔹} : (d ⊓ e) ⊓ (f ⊓ g ⊓((b ⊓ a)⊓ c)) ≤ a :=
by {apply poset_yoneda, intros Γ H, simp only [le_inf_iff] at H,
  repeat{auto_cases}, assumption}
/- Goal state before `assumption`:
[...]
H_right_left_right : Γ ≤ g,
H_right_right_left_left : Γ ≤ b,
H_right_right_left_right : Γ ≤ a
⊢ Γ ≤ a -/
\end{lstlisting}

\paragraph*{Automation and metaprogramming}
A key feature of Lean is that it is its own metalanguage, allowing for seamless in-line definitions of custom tactics. This feature was an invaluable asset, as it allowed the rapid development of a custom tactic library for simulating natural-deduction style proofs inside $\B$ after applying \autoref{poset-yoneda} to insert a slack context variable \lil{Γ}. The preprocessing steps before the call to \lil{assumption} in the previous example are bundled into a single tactic \lil{tidy_context}, and Boolean-valued versions of basic niceties like or-elimination, instantiation of existentials, implication introduction, and even basic automation were easy to write and considerably streamlines the formalization workflow, to the point where the user is able to pretend, with absolute rigor, that they are simply writing proofs in first-order logic while calculations in the complete Boolean algebra are being performed under the hood. \textbf{TODO(jesse) wording}

One use-case where automation is crucial is context-specialization (``change of variables''). For example, if, after preprocessing with \lstinline{poset_yoneda}, the goal is \lstinline{Γ ≤ a ⟹ b}, and one would like to ``introduce the implication'', by adding \lstinline{Γ ≤ a} to context and reducing the goal to \lstinline{Γ ≤ b}, this is impossible as stated. Rather, the deduction theorem lets us rewrite the goal to \lstinline{Γ ⊓ a ≤ b}, and now we may add \lstinline{Γ ⊓ a ≤ a}. So we may introduce the implication after all, but at the cost of specializing the context \lstinline{Γ} to the smaller context \lstinline{Γ' := Γ ⊓ a}. But now, in order for the user to continue the pretense that they are merely doing first-order logic, this change of variables must be propagated to the rest of the assumptions which may still be of the form \lstinline{Γ ≤ _}---which is extremely tedious to do by hand, but easy to automate.

(In a sense, these tactics are only a substitute for a yet-to-be implemented framework for proof by reflection using the Boolean-valued soundness theorem... however, there are important cases which are inaccessible by just re-interpreting proofs of ZF... TODO(jesse) maybe delete)

% (TODO(jesse) add something about how this is an instance of adapting the proof assistant to assimilate a mathematical style of argument, like in the Gonthier paper.)


% % We were also interested in whether the presence of a proof assistant would change the ``user experience'' of working with Boolean-valued models, for one possible complaint about forcing with Boolean-valued models is that calculating truth values inside a complete Boolean algebra is notationally cumbersome in practice. This was indeed our experience---until we adapted Lean's automation to perform much of the bookkeeping for us, to the point where the user is able to pretend, with absolute rigor, that they are simply writing proofs in predicate calculus while lattice operations are being performed under the hood.
% \subsubsection{Automating congruence lemmas with the simplifier}
% The simplifier is one of Lean 3's most sophisticated tools for automation, and is quite powerful when used correctly. It uses automatically-generated congruence lemmas to navigate under binders and perform rewrites using lemmas marked with the \lstinline{simp} attribute. Given the pivotal role which congruence lemmas have in \lstinline{simp}'s functionality, we consider it fitting that \lstinline{simp} allowed us to automate the proofs of the Boolean-valued congruence (sometimes confusingly called \emph{extensionality}) lemmas which are required to speak of a Boolean-valued predicate on a Boolean-valued structure.

% TODO %and also maybe delete, it's not that important. i have the feeling we'll have too much material for 15 pages

\subsection{Check-names}
% \textbf{TODO(jesse) emphasize how check-names are how cardinals and ordinals in the metatheory interact with notions of size internally}
From the definitions of \lil{pSet} and \lil{bSet}, one immediately sees that there is a canonical map \lil{check : pSet → bSet 𝔹}, defined by
\begin{lstlisting}
def check : pSet → bSet 𝔹
| ⟨α,A⟩ := ⟨α, λ a, check (A a), λ a, ⊤⟩
\end{lstlisting}

That is, \lil{check} takes a \lil{pSet} and recursively attaches the Boolean truth-value $\top$ to all elements. We call members of the image of \lil{check} \emph{check-names}. These are also known as \emph{canonical names}, as they are the canonical representation of standard two-valued sets inside a Boolean-valued model of set theory.

One of the most important considerations in forcing is how cardinals and ordinals in the metatheory interact with cardinals and ordinals in the forcing extension. We will see later that after translating the entire forcing argument to type theory, the presence of \lstinline{pSet} is misleading. It is not the ordinals and cardinals inside \lstinline{pSet} which are of fundamental importance, but rather the ordinals and cardinals of Lean itself. The role of the check-names is not any less decisive, but is it clear that the check-names are merely the way in which the metatheory's cardinals and ordinals interact with that in the forcing extension. \lstinline{pSet} is not a prerequisite for studying \lstinline{bSet} or performing forcing, but only a convenient aid to organize information about the check-names.

\subsection{Transfinite induction}
\label{sec:org9e70de8}
In set theory, it is common to prove propositions via induction on an ordinal-valued rank function. In fact, this is how V\(^{\text{BB}}\) is typically constructed, by induction on the rank of sets in an existing universe of sets V. In Lean, this style of argument does not come for as free as, say, structural induction principles like $\in$-induction, which by virtue of the construction of bSet BB, \emph{is} the induction principle for that inductive type. However, an interface is available for well-founded recursion on well-founded relations, and a development of the theory of ordinals as equivalence-classes of well-ordered types is available in \texttt{mathlib}. There were two places in the present work where transfinite induction was unavoidable, namely in the construction of an antichain for the maximum principle, and the verification that the canonical embedding of ordinals into \texttt{pSet} is injective.

\subsection{The fundamental theorem of forcing}

The fundamental theorem of forcing for Boolean-valued models \cite{hamkins-seabold1} states that for any complete Boolean algebra $B$, $V^B$ is a Boolean-valued model of ZFC. Since, in type theory, a type universe \lstinline{Type u} takes the place of the standard universe $V$, the analogous statement in our setting is that for every complete Boolean algebra $\B$, \lstinline{bSet 𝔹} is a Boolean-valued model of ZFC.

After the development of the custom proof language,
\textbf{(TODO(jesse) add more definitions to ref to}
much of the verification of the axioms besides choice is routine, as the user is able to pretend they are working in ordinary 2-valued logic. We describe some aspects of \lil{bSet 𝔹} which are illuminated by the verification of the axioms and which will be important for forcing $\neg\mathsf{CH}$.

\paragraph*{The axiom of infinity}
$\omega$ \lil{: bSet 𝔹} is $\check{\omega}$. $\omega$ is defined in \lil{pSet} to be the collection of all finite von Neumann ordinals, which are defined by induction on $\mathbb{N}$. While it is easy to show $\check{\omega}$ satisfies the axiom of infinity
\begin{lstlisting}
def axiom_of_infinity_spec (u : bSet 𝔹) : 𝔹 :=
  (∅∈ᴮ u) ⊓ (⨅i_x, ⨆i_y, (u.func i_x ∈ᴮ u.func i_y))
\end{lstlisting}
it can furthermore be shown to satisfy the universal property of $\omega$, which says that $\omega$ is a subset of any set which contains $\emptyset$ and is closed under the successor operation $x \mapsto x \cup {x}$.

\paragraph*{The axiom of powerset}
\begin{definition}
  Fix a $\B$-valued set \lil{x = ⟨α, A, b⟩}. Let \lil{χ : α → 𝔹} be a function. The subset of \lil{x} associated to \lil{χ} is a \lil{𝔹}-valued set defined as follows:
  \begin{lstlisting}
def set_of_indicator {x} (χ : x.type → 𝔹) := ⟨x.type, x.func, χ⟩
\end{lstlisting}

The \textbf{powerset} $\mathcal{P}(x)$ of $x$ is defined to be the following \lil{𝔹}-valued set, whose underlying type is the type of all functions \lil{x.type → 𝔹}:
\begin{lstlisting}
def bv_powerset (u : bSet 𝔹) : bSet 𝔹 :=
⟨u.type → 𝔹, λ f, set_of_indicator f, λ f, set_of_indicator f ⊆ᴮ u⟩
\end{lstlisting}
\end{definition}

\paragraph*{The axiom of choice}
Following the presentation in Bell \cite{bell1}, we verified Zorn's lemma, which is provably equivalent over $\mathsf{ZF}$ to the axiom of choice. As is the case with \lil{pSet}, establishing the axiom of choice requires the use of a choice principle from the metatheory. This was the most involved part of our verification of the fundamental theorem of forcing, and relies on the technical tool of \emph{mixtures}, which allow sequences of $\B$-valued sets to be ``averaged'' into new ones, and the \emph{maximum principle}, which allows existentially quantified statements to be instantiated without changing their truth-value.

\paragraph*{The smallness of $\B$}
Before ending this section, we remark that the ``smallness'' (or more precisely, the fact that $\B$ lives in the same universe of types out of which \lil{bSet 𝔹} is being built), plays a crucial a role in making \lstinline{bSet 𝔹} a model of ZFC. It is required for extracting the witness needed for the maximum principle, and is also required to even define the powerset operation, because the underlying type of the powerset is the function type of all maps into \lstinline{𝔹}.

\section{Forcing}
\label{sect:forcing}

% As we emphasized in \autoref{sect:outline}, the real challenge with the forcing argument lies in selecting a suitable $\B$ and reasoning about how its structure affects the structure of \lstinline{bSet 𝔹}. Since we are particularly interested in the behavior of cardinalities in \lstinline{bSet 𝔹},
\subsection{Representing Lean's ordinals inside \lil{pSet} and \lil{bSet}}
The treatment of ordinals in \lil{mathlib} associates a class of ordinals to every type universe, defined as isomorphism classes of well-ordered types, and includes interfaces for both well-founded and transfinite recursion. Lean's ordinals may be represented inside \lil{pSet} by defining a map \lil{ordinal.mk : ordinal → pSet} via transfinite recursion; it is nothing more than the von Neumann definition of ordinals. In pseudocode,
\begin{lstlisting}
def ordinal.mk : ordinal → pSet
| 0 := ∅
| succ ξ := pSet.succ (ordinal.mk ξ) -- (mk ξ ∪ {mk ξ})
| is_limit ξ := ⋃ η < ξ, (ordinal.mk η)
\end{lstlisting}
Composing by \lil{check} (\autoref{def-check}) yields a map \lil{check ∘ ordinal.mk : ordinal → bSet 𝔹}. (We could just as well defined \lstinline{ordinal.mk' : ordinal → bSet 𝔹} analogously to \lstinline{ordinal.mk} such that \lstinline{ordinal.mk' = check ∘ ordinal.mk}; the point is that there is a link between the metatheory's notion of size and order with that of the forcing extension.)

Cardinals are defined separately from ordinals as bijective equivalence classes of types, but are canonically represented by ordinals which is are not bijective with any predecessor. We let \lil{aleph : ordinal → ordinal} index these representatives. For the rest of this section, unadorned alephs (e.g. ``\lil{ℵ₂}'') will mean either an ordinal of the form \lil{aleph ξ} or a choice of representative from the isomorphism class of well-ordered types, and checked alephs (e.g. ``\lil{ℵ₂̌ }'') will mean the \lil{check ∘ ordinal.mk} of that ordinal.

\subsection{The Cohen poset and the regular open algebra}
Forcing with partial orders and forcing with complete Boolean algebras are related by the fact that every poset of forcing conditions can be embedded into a complete Boolean algebra as a dense suborder. This will be the case for our forcing argument: our Boolean algebra is the algebra of regular opens on $2^{\aleph_2 \times \mathbb{N}}$, which embeds the poset of forcing conditions typically used for Cohen forcing as a dense suborder.
\begin{definition}
  The \textbf{Cohen poset} for adding $\aleph_2$-many Cohen reals is the collection of all finite partial functions $\aleph_2 \times \mathbb{N} \to \mathbf{2}$, ordered by reverse inclusion.
\end{definition}

In the formalization, the Cohen poset is represented as a \lstinline{structure} with three fields:
\begin{lstlisting}
structure 𝒞 : Type :=
  (ins : finset (ℵ₂.type × ℕ))
  (out : finset (ℵ₂.type × ℕ))
  (H : ins ∩ out = ∅)
\end{lstlisting}

That is, we identify a finite partial function $f$ with the triple \lil{⟨f.ins, f.out, f.H⟩}, where \lil{f.ins} is the preimage of $\{1\}$, \lil{f.out} is the preimage of $\{0\}$, and \lil{f.H} ensures well-definedness. While $f$ is usually defined as a finite partial function, we found that in practice $f$ is really only needed to give a finite partial specification of a subset of $\aleph_2 \times \mathbb{N}$ (i.e. a finite set \lil{f.ins} which \emph{must} be in the subset, and a finite set \lil{f.out} which \emph{must not} be in the subset), and chose this representation to make that information immediately accessible.

\begin{definition}
  Let $X$ be a topological space, and for any open set $U$, let $U^\perp$ denote the complement of the closure of $U$. The \textbf{regular open algebra} of a topological space $X$, written $\operatorname{RO}(X)$, is the collection of all open sets $U$ such that $U = (U^\perp)^\perp$, equipped with the structure of a complete Boolean algebra, with $x \sqcap y := x \cap y$, $x \sqcup y := ((x \sqcup y)^\perp)^\perp$, $\neg x := x^\perp$, and $\bigsqcup x_i := ((\bigcup x_i)^\perp)^\perp$.
\end{definition}

The Boolean algebra which we will use for forcing $\neg\mathsf{CH}$ is $\operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$. Unless stated otherwise, for the rest of this section, we put $\B := \operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$.

\begin{definition}
  We define the \textbf{canonical embedding} of the Cohen poset into $\B$ as follows:
  \begin{lstlisting}
def ι : 𝒞 → 𝔹 := λ p, {S | p.ins ⊆ S ∧ p.out ⊆ - S}    
\end{lstlisting}
\end{definition}
That is, we send each \lil{c : 𝒞} all the subsets which satisfy the specification given by \lil{c}. This is a clopen set, hence regular. Crucially, this embedding is \emph{dense}:
\begin{lstlisting}
lemma 𝒞_dense {b : 𝔹} (H : ⊥ < b) : ∃ p : 𝒞, ι p ≤ b  
\end{lstlisting}
Recalling that $\leq$ in $\B$ is subset-inclusion, we see that this is essentially because the image of $\iota : \mathcal{C} \to \B$ \emph{is} is the standard basis for the product topology. Our chosen encoding of the Cohen poset also made it easier to perform this identification when formalizing this proof.
% The formalization of definitions and lemmas displayed in this section required nontrivial extensions to \lil{mathlib}'s library on topological spaces, including a complete description of the standard basis of the product topology on spaces of the form $2^I$. We refer the interested reader to our formalization for more details.
\subsection{Adding $\aleph_2$-many distinct Cohen reals} \label{subsect:cohen-reals}
As we saw in \autoref{def-powerset}, for any $\B$-valued set $x$, characteristic functions into $\B$ from the underlying type of $x$ determine $\B$-valued subsets of $x$. While the ingredients $\aleph_2$ and $\mathbb{N}$ for $\B$ are types and thus external to \lil{bSet 𝔹}, they are represented nonetheless inside \lil{bSet 𝔹} by their check-names $\check{\aleph_2}$ and $\check{\mathbb{N}}$, and in fact \lil{ℵ₂} \emph{is} \lil{ℵ₂̌ .type} and \lil{ℕ} \emph{is} $\check{\mathbb{N}}$\lil{.type}. Given our specific choice of $\B$, this will allow us to construct an $\aleph_2$-indexed family of distinct subsets of $\check{\N}$, which we can then convert into an injective function from \lil{ℵ₂̌ } to \lil{ℕ}, \emph{inside} \lil{bSet 𝔹}.

\begin{definition}
  Let $\nu : \aleph_2$. For any $n : \N$, the collection of all subsets of $\aleph_2 \times \N$ which contain $(\nu, n)$ is a regular open of $2^{\aleph_2 \times \N}$, called the \textbf{principal open} $\mathbf{P}_{(\nu, n)}$ over $(\nu, n)$.
\end{definition}

\begin{definition}
  Let $\nu : \aleph_2$. We associate to $\nu$ the $\B$-valued characteristic function $\chi_{\nu} : \N \to \B$ defined by $\chi_{\nu}(n) := \mathbf{P}_{(\nu, n)}$. In light of our previous observations, we see that each $\chi_{\nu}$ induces a new $\B$-valued subset $\widetilde{\chi_{\nu}} \subseteq \check{\N}$. We call $\widetilde{\chi_{\nu}}$ a \textbf{Cohen real}.
\end{definition}
This gives us an $\aleph_2$-indexed family of Cohen reals. Converting this data into an injective function from \lil{ℵ₂̌ } to \lil{ℕ} inside \lil{bSet 𝔹} requires some care. One must check that $\nu \mapsto \widetilde{\chi_{\nu}}$ is externally injective, and this is where the characterization of the Cohen poset as a dense subset of $\B$ (and moving back and forth between this representation and the definition as finite partial functions) comes in. Furthermore, one has to develop machinery similar to that for the powerset operation to convert an external injective function \lstinline{x.type → bSet 𝔹} to a $\B$-valued set which \lstinline{bSet 𝔹} believes is a injective function, while maintaining conditions on the intended codomain. Our custom tactics and automation for reasoning inside $\B$ made this latter task significantly easier than it would have been otherwise. We refer the interested reader to our formalization for details.

\subsection{Preservation of cardinal inequalities} \label{subsect:cardinal-inequalities}
So far, we have shown that for $\B = \operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$, \lil{bSet 𝔹} thinks \lstinline{ℵ₂̌ } is smaller than $\mathcal{P}(\check{\mathbb{N}})$.

Although Lean believes there is a strict inequality of cardinals $\aleph_0 < \aleph_1 < \aleph_2$, in general we can only deduce that their representations inside \lil{bSet 𝔹} are subsets of each other: $\top \leq \check{\aleph_0} \subseteq^\B \check{\aleph_1} \subseteq^\B \check{\aleph_2}$. To finish negating $\mathsf{CH}$, it suffices to show that \lstinline{bSet 𝔹} believes $\check{\aleph_0}$ is strictly smaller than $\check{\aleph_1}$, and that \lstinline{bSet 𝔹} believes $\check{\aleph_1}$ is a strictly smaller than $\check{\aleph_2}$. That is, we want that the passage from $\aleph_i$ to $\check{\aleph_i}$ preserves cardinal inequalities.

\begin{definition}
  For our purposes, ``strictly smaller'' will mean ``there exists no function \lil{f} such that for every \lil{v ∈ y}, there exists a \lil{w ∈ x} such that \lil{(w,v) ∈ f}''. Translated to a Boolean truth-value (with the definition of a function abbreviated), this means: the Boolean truth-value of ``\lil{x} is strictly smaller than \lil{y}'' is defined to be
\begin{center}\lstinline{-(⨆f, (is_func f) ⊓ ⨅v, v ∈ᴮ y ⟹ ⨆w, w ∈ᴮ x ⊓ pair w v ∈ᴮ f)}\end{center}
\end{definition}

The condition on an arbitrary $\B$ which ensures the preservation of cardinal inequalities is the \emph{countable chain condition}.

\begin{definition}
We say that $\B$ has the \textbf{countable chain condition} (CCC) if every antichain $\mathcal{A} : I \to \B$ (i.e. an indexed collection of elements $\mathcal{A} := \{a_i\}$ such that whenever $i \neq j, a_i \sqcap a_j = \bot$) has a countable image.
\end{definition}

We sketch the argument that CCC implies the preservation of cardinal inequalities. The proof is by contraposition. Let $\kappa_1$ and $\kappa_2$ be cardinals such that $\kappa_1 < \kappa_2$, and suppose that $\check{\kappa_1}$ is not strictly smaller than $\check{\kappa_2}$. Then there exists some \lil{f : bSet 𝔹} and some $\Gamma > \bot$ such that \lstinline{Γ ≤ (is_func f) ⊓ ⨅v, v ∈ᴮ κ₁̌  ⟹ ⨆w, w ∈ᴮ κ₂̌  ⊓ (w,v) ∈ᴮ f}. Then one can show:
\begin{lstlisting}
lemma AE_of_check_larger_than_check :
∀ β < κ₂, ∃ η < κ₁, ⊥ < (is_func f) ⊓ (η̌, β̌ ) ∈ᴮ f
\end{lstlisting}
The name of this lemma emphasizes that what was happened here is that, given this $f$ and the assumption that it satisfes some $\forall$-$\exists$ formula inside \lil{bSet 𝔹}, we are able to extract, by virtue of $\check{\kappa_1}$ and $\check{\kappa_2}$ being check-names, a $\forall$-$\exists$ statement in the \emph{metatheory}. Using Lean's choice principle, we can then convert this $\forall$-$\exists$ statement into a function $g : \kappa_2 \to \kappa_1$, such that for every $\beta$, \lstinline{⊥ < (is_func f) ⊓ (g(β)̌ , β̌ ) ∈ᴮ f}. Since $\kappa_2 > \kappa_1$, it follows from the infinite pigeonhole principle that there exists some $\eta < \kappa_1$ such that the $g^{-1}(\{\eta\})$ is uncountable. Define $\mathcal{A} : g^{-1}(\{\eta\}) \to \B$ by $\mathcal{A}(\beta) :=$ \lil{(is_func f) ⊓ (g(β)̌ , β̌ ) ∈ᴮ f}. This is an uncountable antichain because if $\beta_1 \neq \beta_2$, then the well-definedness part of \lil{is_func f} ensures that, because $g(\beta_1) = g(\beta_2)$, the truth-value $\check{\beta_1} = f(g(\beta_1)) \neq^\B f(g(\beta_2)) = \check{\beta_2}$ is $\bot$.

Thus, conditional on showing that $\B = \operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$ has the CCC, we now have that cardinal inequalities are preserved in \lstinline{bSet 𝔹}. Combining this with the injection \lil{ℵ₂̌  → 𝒫(ℕ)}, we obtain:
\begin{lstlisting}
theorem neg_CH : ⊤ ≤ ℕ ≺ (ℵ₁)̌  ≺ (ℵ₂)̌  ≼ 𝒫(ℕ)
\end{lstlisting}

The arguments sketched in \autoref{subsect:cohen-reals} and \autoref{subect:cardinal-inequalities} form the heart of the forcing argument. Their proofs involve taking objects in \lil{Type u} and \lil{bSet 𝔹}, constructing corresponding objects on the other side, and reasoning about them in ordinary and $\B$-valued logic simultaneously to determine cardinalities in \lstinline{bSet 𝔹}. We have omitted many details from our discussion, but of course, all details of the proofs have been formally verified.

\section{Transfinite combinatorics and the countable chain condition}
\label{sect:ccc}
What remains now is to prove that $\operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$ has the CCC. There are several ways forward, but we chose the most general, anticipating its usefulness in future formalizations of set theory and forcing.

TODO(floris)
\subsection{The $\Delta$-system lemma}
TODO(floris)
\subsection{$\operatorname{RO}(2^{\aleph_2 \times \mathbb{N}})$ has the countable chain condition}
TODO(floris)

\section{Conclusions and future work}
\subsection{Proof by reflection}

Combined with the usual completeness theorem, the Boolean-valued soundness theorem will allow us to prove statements about a structure of the form bSet BB as follows: if the statement $\phi$ is provable from ZF, then we may prove that ZF proves $\phi$ by applying the completeness theorem to reason inside an arbitrary model of ZF. This avoids the complications of trying to work directly inside Boolean-valued logic. Then, given a Boolean-valued L\(_{\text{ZFC}}\)-structure $\mathbf{M}$ which satisfies ZF, the Boolean-valued soundness theorem tells us that this proof may be replayed inside $\mathbf{M}$ so that $\phi$ has truth-value greater than the truth-values of ZF, and is therefore satisfied inside $\mathbf{M}$.

In this way, we can transport proofs of statements such as "Zorn's lemma is equivalent to the axiom of choice", which is provable from ZF, directly from the world of 2-valued models to the world of Boolean-valued models.

We also remark that while our use of \texttt{simp} lemmas to generate congruence certificates sufficed for the purposes of this work, the "real" proof that something like the subset predicate is $=^B$-extensional is a proof by reflection: one constructs a formula which reifies the predicate, and then applies the fact that one is in the deeply-embedded Boolean-valued structure to obtain the congruence lemma automatically. We also intend to automate this.
\subsection{Forcing with generic models}

Our method does not support iterated forcing. Our method starts with a universe of types and uses that to construct a model of set theory.
\subsection{Towards a formal proof of the independence of the continuum hypothesis}


This work was carried out as part of the Flypitch project, which aims to formalize the independence of the continuum hypothesis from ZFC, i.e. that CH and its negation are both unprovable from the ZFC axioms.

Future goals of the project include:

\begin{itemize}
\item Various formulations of the axioms of ZFC are equiconsistent, including the versions used in this paper
\end{itemize}

\begin{itemize}
\item In order to complete this formalization, we had to develop several libraries, e.g. for dependently-typed vectors, significant extensions of the lattice and Boolean algebra library, the theory of product topological space and their bases, and extensions to the set theory and ordinal libraries.
  
\item \texttt{pSet} was not essential. Rather, in our type-theoretic foundations, the construction of a Boolean-valued standard universe of set theory has equal footing with the construction of an ordinary standard universe of set theory. We see that for the purposes of working with V\(^{\text{BB}}\), V is no longer a prerequisite, but merely a useful tool for organizing the check-names.
\item We used several features of our type-theoretic foundations to our advantage. We constructed a standard universe of set theory structurally in such a way that many properties of the underlying universe of types are reflected inside the model of set theory, and such that we get the axiom of regularity (more precisely, the principle of epsilon-induction) for free as the automatically-generated induction principle for our inductive type.
\item Lean is great, meteoric growth --- remark on recentness of developments in \texttt{mathlib} which made this possible (acknowledge developments from other theorem-provers, including porting of libraries e.g. \texttt{lattice} from Isabelle).
\item Evidence that formalized mathematics is ready "in the large"
\end{itemize}

\section{References}
\begin{itemize}
\item Moore's The method of forcing
\item Halmos-Givant Textbook on boolean algebras
\item Gunther Pagano et al forcing in Isabelle/ZF
\item Paulson constructible universe and set theory in Isabelle/ZF
\item Sets in Coq, Coq in Sets
\item Sets in types, types in sets
\item Aczel's encoding of ZFC inside type theory
\end{itemize}

\end{document}